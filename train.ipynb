{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import imutils\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.models import vgg16, vgg16_bn\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from models import *\n",
    "from func import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "device = \"cpu\"\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),  # only resize the shorter edge to keep aspect ratio\n",
    "    transforms.CenterCrop(256),  # then crop to get square img\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Does not resize style image, since gram matrices has the same shapes after all\n",
    "style_transform = transforms.Compose([ \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "dataset = ImageFolder(\"data\", transform=transform)\n",
    "loader = DataLoader(dataset, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = Vgg16Wrapper(requires_grad=False)\n",
    "vgg.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(\"data/starry_night.jpg\")\n",
    "img.thumbnail((256, 256), Image.ANTIALIAS)\n",
    "style_img = style_transform(img).to(device)\n",
    "style_img = style_img.unsqueeze(0)\n",
    "\n",
    "# Extract style information from style image\n",
    "_, style_acts_true = vgg(style_img,\n",
    "                    style_layer_idxs=[5, 12, 22, 32])\n",
    "grams_true = [gram_matrix(act) for act in style_acts_true]\n",
    "grams_true = [gram.repeat(batch_size, 1, 1) for gram in grams_true]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1: content=5599.5352, style=1228.4926, total=6828.0278\n",
      "Iter 2: content=5219.8799, style=4426.1997, total=9646.0801\n",
      "Iter 3: content=5077.5112, style=12195.6289, total=17273.1406\n",
      "Iter 4: content=3574.9048, style=3431.7566, total=7006.6611\n",
      "Iter 5: content=3384.4060, style=3488.7773, total=6873.1836\n",
      "Iter 6: content=4312.9668, style=3386.7637, total=7699.7305\n",
      "Iter 7: content=4938.4268, style=3640.4358, total=8578.8623\n",
      "Iter 8: content=4637.3560, style=3175.1948, total=7812.5508\n",
      "Iter 9: content=3014.4910, style=3417.9067, total=6432.3975\n",
      "Iter 10: content=3741.4185, style=3246.7822, total=6988.2007\n",
      "Iter 11: content=5102.7153, style=3090.0229, total=8192.7383\n",
      "Iter 12: content=5209.4907, style=3342.4006, total=8551.8916\n",
      "Iter 13: content=3857.4368, style=3192.2092, total=7049.6460\n",
      "Iter 14: content=4800.1089, style=2972.0105, total=7772.1191\n"
     ]
    }
   ],
   "source": [
    "content_weight = 1e5\n",
    "style_weight = 1e10\n",
    "\n",
    "num_epochs = 2\n",
    "ckpt_interval = 1#200\n",
    "ckpt_dir = \"checkpoints\"\n",
    "result_dir = \"gen_imgs\"\n",
    "Path(ckpt_dir).mkdir(exist_ok=True)\n",
    "Path(result_dir).mkdir(exist_ok=True)\n",
    "\n",
    "net = TransformerNet()\n",
    "net.to(device)\n",
    "\n",
    "opt = optim.Adam(net.parameters(), lr=1e-3)\n",
    "mse = nn.MSELoss()\n",
    "\n",
    "writer = SummaryWriter('logdir')\n",
    "i = 1\n",
    "\n",
    "for _ in range(num_epochs):\n",
    "    for X_train, _ in loader:\n",
    "        opt.zero_grad()\n",
    "        \n",
    "        X_train = X_train.to(device)\n",
    "        gen_imgs = net(X_train)\n",
    "        \n",
    "        # Extract content & style features from generated img\n",
    "        norm_gen_imgs = normalize_images(gen_imgs)\n",
    "        content_pred, style_acts = vgg(norm_gen_imgs, content_layer_idxs=[12], \n",
    "                                       style_layer_idxs=[5, 12, 22, 32])\n",
    "        grams_pred = [gram_matrix(act) for act in style_acts]\n",
    "        # Extract content features from content images (the inputs in the 1st place)\n",
    "        content_true, _ = vgg(X_train, content_layer_idxs=[12])\n",
    "        \n",
    "        content_loss = 0.\n",
    "        for c_pred, c_true in zip(content_pred, content_true):\n",
    "            content_loss += mse(c_pred, c_true)\n",
    "        content_loss *= content_weight\n",
    "            \n",
    "        style_loss = 0.\n",
    "        for g_pred, g_true in zip(grams_pred, grams_true):\n",
    "            style_loss += mse(g_pred, g_true)\n",
    "        style_loss *= style_weight\n",
    "        \n",
    "        total_loss = content_loss + style_loss\n",
    "        total_loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        writer.add_scalar('content_loss', content_loss.item())\n",
    "        writer.add_scalar('style_loss', style_loss.item())\n",
    "        writer.add_scalar('total_loss', total_loss.item())\n",
    "        \n",
    "        if i % ckpt_interval == 0:\n",
    "            # Save model\n",
    "            torch.save(net.state_dict(), f\"{ckpt_dir}/net_{i}.pth\")\n",
    "            # Save a generated images\n",
    "            with open(f\"{result_dir}/generated_{i}.jpg\", \"w\") as f:\n",
    "                save_image(gen_imgs, f)\n",
    "            \n",
    "        print(f\"Iter {i}: content={content_loss.item():.4f}, style={style_loss.item():.4f}, total={total_loss.item():.4f}\")\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml]",
   "language": "python",
   "name": "conda-env-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
